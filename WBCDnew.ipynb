{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#point d\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "#importing wisconsin brest cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "z = data.target\n",
    "\n",
    "#splitting the dataset into training and testing\n",
    "X_train, X_test, z_train, z_test = train_test_split(X, z, test_size=0.2)\n",
    "\n",
    "z_train = z_train.reshape(-1,)\n",
    "z_test = z_test.reshape(-1,)\n",
    "\n",
    "#transformation\n",
    "def transformation(z, num_classes = 2):\n",
    "    z = np.eye(num_classes)[z]\n",
    "    return z\n",
    "\n",
    "#inverse\n",
    "def inverse_transformation(z):\n",
    "    z = np.argmax(z, axis = 1)\n",
    "    return z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define RELU function as the activation function for the hidden layer\n",
    "def relu(x):\n",
    "    return np.maximum(0,x)\n",
    "\n",
    "#define softmax as the activation function for the output layer\n",
    "def softmax(a):\n",
    "    expA = np.exp(a)\n",
    "    return expA / expA.sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialization of weight and biases for a flexible number of hidden layers including the output layer\n",
    "def initialize_parameters(layer_dims):\n",
    "    parameters = {}\n",
    "    for i in range(1, len(layer_dims)):\n",
    "        parameters[f'W{i}'] = np.random.randn(layer_dims[i-1], layer_dims[i]) * 0.01\n",
    "        parameters[f'b{i}'] = np.zeros((layer_dims[i], 1))\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We now write the feed forward pass\n",
    "def feed_forward(X, parameters):\n",
    "    cache = {'A0': X}\n",
    "    for i in range(1, len(parameters)//2 + 1):\n",
    "        Z = cache[f'A{i-1}'] @ parameters[f'W{i}'] + parameters[f'b{i}'].T\n",
    "        A = relu(Z) if i < len(parameters)//2 else softmax(Z) #if we are at the output layer, we use softmax\n",
    "        cache[f'Z{i}'] = Z\n",
    "        cache[f'A{i}'] = A\n",
    "    return cache\n",
    "\n",
    "def feed_forward_hidden(X, parameters):\n",
    "    cache = {'A0': X}\n",
    "    for i in range(1, len(parameters)//2):  # s'arrête avant la couche de sortie\n",
    "        Z = cache[f'A{i-1}'] @ parameters[f'W{i}'] + parameters[f'b{i}']\n",
    "        A = relu(Z)\n",
    "        cache[f'Z{i}'] = Z\n",
    "        cache[f'A{i}'] = A\n",
    "    return cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we now define the cost/loss function. We use the cross-entropy because it is a classification problem\n",
    "def cross_entropy(prediction, target):\n",
    "    epsilon = 1e-12 #added to avoid log(0)\n",
    "    return -(target*np.log(prediction+epsilon) + (1-target)*np.log(1-prediction+epsilon))\n",
    "\n",
    "def d_cross_entropy(prediction, target):   #attention: retrouver dérivée\n",
    "    epsilon = 1e-12 #added to avoid log(0)\n",
    "    return -(target/(prediction+epsilon) - (1-target)/(1-prediction+epsilon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagation(X, Y, cache, parameters):\n",
    "    m = Y.shape[0]\n",
    "    gradients = {}\n",
    "    output_error = d_cross_entropy(cache[f'A{len(parameters)//2}'], transformation(Y))\n",
    "    for i in range(len(parameters)//2, 0, -1):\n",
    "        gradients[f'dW{i}'] = 1/m * cache[f'A{i-1}'].T@ output_error\n",
    "        gradients[f'db{i}'] = 1/m * np.sum(output_error, axis=0, keepdims=True)\n",
    "        output_error = np.dot(output_error, parameters[f'W{i}'].T) * (cache[f'Z{i-1}'] > 0) if i > 1 else None\n",
    "    return gradients\n",
    "\n",
    "def backpropagation_update_parameters(parameters, gradients, learning_rate):\n",
    "    for i in range(1, len(parameters)//2 + 1):\n",
    "        parameters[f'W{i}'] -= learning_rate * gradients[f'dW{i}']\n",
    "        parameters[f'b{i}'] -= learning_rate * gradients[f'db{i}'].T\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at iteration 0 0.7037857884801421\n",
      "Cost at iteration 100 0.6794227612621286\n",
      "Cost at iteration 200 0.6706375069964506\n",
      "Cost at iteration 300 0.6647091089988795\n",
      "Cost at iteration 400 0.6607046686842254\n",
      "Cost at iteration 500 0.6579995658900458\n",
      "Cost at iteration 600 0.6561729566248524\n",
      "Cost at iteration 700 0.65494037653712\n",
      "Cost at iteration 800 0.6541092914856429\n",
      "Cost at iteration 900 0.6535493669179505\n",
      "Cost at iteration 1000 0.6531724175324418\n",
      "Cost at iteration 1100 0.6529188265719973\n",
      "Cost at iteration 1200 0.6527483303901718\n",
      "Cost at iteration 1300 0.6526337633485467\n",
      "Cost at iteration 1400 0.6525568146072902\n",
      "Cost at iteration 1500 0.6525051527392971\n",
      "Cost at iteration 1600 0.6524704796747817\n",
      "Cost at iteration 1700 0.6524472153038182\n",
      "Cost at iteration 1800 0.6524316094488577\n",
      "Cost at iteration 1900 0.6524211430275649\n",
      "Cost at iteration 2000 0.6524141246323396\n",
      "Cost at iteration 2100 0.6524094189930358\n",
      "Cost at iteration 2200 0.6524062643451913\n",
      "Cost at iteration 2300 0.6524041496728118\n",
      "Cost at iteration 2400 0.6524027322405118\n",
      "Cost at iteration 2500 0.6524017822168244\n",
      "Cost at iteration 2600 0.6524011455030408\n",
      "Cost at iteration 2700 0.6524007187901493\n",
      "Cost at iteration 2800 0.6524004328255645\n",
      "Cost at iteration 2900 0.6524002411898661\n",
      "Cost at iteration 3000 0.6524001127705156\n",
      "Cost at iteration 3100 0.6524000267154894\n",
      "Cost at iteration 3200 0.6523999690501009\n",
      "Cost at iteration 3300 0.6523999304090756\n",
      "Cost at iteration 3400 0.6523999045163661\n",
      "Cost at iteration 3500 0.6523998871662405\n",
      "Cost at iteration 3600 0.652399875540391\n",
      "Cost at iteration 3700 0.6523998677502689\n",
      "Cost at iteration 3800 0.6523998625303737\n",
      "Cost at iteration 3900 0.6523998590327136\n",
      "Cost at iteration 4000 0.6523998566890673\n",
      "Cost at iteration 4100 0.6523998551186848\n",
      "Cost at iteration 4200 0.6523998540664375\n",
      "Cost at iteration 4300 0.6523998533613719\n",
      "Cost at iteration 4400 0.6523998528889386\n",
      "Cost at iteration 4500 0.6523998525723822\n",
      "Cost at iteration 4600 0.6523998523602723\n",
      "Cost at iteration 4700 0.652399852218147\n",
      "Cost at iteration 4800 0.6523998521229157\n",
      "Cost at iteration 4900 0.6523998520591056\n",
      "Cost at iteration 5000 0.6523998520163494\n",
      "Cost at iteration 5100 0.6523998519877005\n",
      "Cost at iteration 5200 0.6523998519685041\n",
      "Cost at iteration 5300 0.6523998519556417\n",
      "Cost at iteration 5400 0.6523998519470231\n",
      "Cost at iteration 5500 0.6523998519412483\n",
      "Cost at iteration 5600 0.6523998519373788\n",
      "Cost at iteration 5700 0.6523998519347859\n",
      "Cost at iteration 5800 0.6523998519330487\n",
      "Cost at iteration 5900 0.6523998519318848\n",
      "Cost at iteration 6000 0.6523998519311046\n",
      "Cost at iteration 6100 0.6523998519305818\n",
      "Cost at iteration 6200 0.6523998519302319\n",
      "Cost at iteration 6300 0.6523998519299974\n",
      "Cost at iteration 6400 0.65239985192984\n",
      "Cost at iteration 6500 0.6523998519297347\n",
      "Cost at iteration 6600 0.6523998519296641\n",
      "Cost at iteration 6700 0.6523998519296168\n",
      "Cost at iteration 6800 0.6523998519295852\n",
      "Cost at iteration 6900 0.6523998519295636\n",
      "Cost at iteration 7000 0.6523998519295497\n",
      "Cost at iteration 7100 0.6523998519295401\n",
      "Cost at iteration 7200 0.6523998519295336\n",
      "Cost at iteration 7300 0.6523998519295294\n",
      "Cost at iteration 7400 0.6523998519295267\n",
      "Cost at iteration 7500 0.6523998519295247\n",
      "Cost at iteration 7600 0.6523998519295234\n",
      "Cost at iteration 7700 0.6523998519295225\n",
      "Cost at iteration 7800 0.6523998519295219\n",
      "Cost at iteration 7900 0.6523998519295214\n",
      "Cost at iteration 8000 0.6523998519295211\n",
      "Cost at iteration 8100 0.6523998519295211\n",
      "Cost at iteration 8200 0.6523998519295209\n",
      "Cost at iteration 8300 0.652399851929521\n",
      "Cost at iteration 8400 0.6523998519295208\n",
      "Cost at iteration 8500 0.6523998519295207\n",
      "Cost at iteration 8600 0.6523998519295208\n",
      "Cost at iteration 8700 0.6523998519295208\n",
      "Cost at iteration 8800 0.6523998519295207\n",
      "Cost at iteration 8900 0.6523998519295208\n",
      "Cost at iteration 9000 0.6523998519295207\n",
      "Cost at iteration 9100 0.6523998519295207\n",
      "Cost at iteration 9200 0.6523998519295207\n",
      "Cost at iteration 9300 0.6523998519295208\n",
      "Cost at iteration 9400 0.6523998519295207\n",
      "Cost at iteration 9500 0.6523998519295207\n",
      "Cost at iteration 9600 0.6523998519295208\n",
      "Cost at iteration 9700 0.6523998519295207\n",
      "Cost at iteration 9800 0.6523998519295207\n",
      "Cost at iteration 9900 0.6523998519295208\n",
      "Accuracy on test set: 0.5701754385964912\n"
     ]
    }
   ],
   "source": [
    "#inizializzo dimensioni layers\n",
    "n_inputs, n_features = X_train.shape\n",
    "hid = 7 #number of neurons in the hidden layers\n",
    "out = 2 #number of neurons in the output layer\n",
    "layer_dims = [n_features, hid, out] #for one hidden layer\n",
    "#layer_dims = [n_features, hid, hid, out] for two hidden layers\n",
    "\n",
    "#inizializzo parametri\n",
    "parameters = initialize_parameters(layer_dims)\n",
    "\n",
    "#initialization of hyperparameters\n",
    "learning_rate = 1e-3\n",
    "epochs = 10000\n",
    "\n",
    "#training\n",
    "for i in range(epochs):\n",
    "    cache = feed_forward(X_train, parameters)\n",
    "    gradients = backpropagation(X_train, z_train, cache, parameters)\n",
    "    parameters = backpropagation_update_parameters(parameters, gradients, learning_rate)\n",
    "    if i % 100 == 0:\n",
    "        print(f'Cost at iteration {i}', cross_entropy(cache[f'A{len(parameters)//2}'], transformation(z_train)).mean())\n",
    "\n",
    "#testing\n",
    "cache = feed_forward(X_test, parameters)\n",
    "predictions = cache[f'A{len(parameters)//2}']\n",
    "predictions = np.round(predictions)\n",
    "accuracy = accuracy_score(z_test, inverse_transformation(predictions))\n",
    "print('Accuracy on test set:', accuracy)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
